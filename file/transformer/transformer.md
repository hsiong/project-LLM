# Transformer

基于自注意力机制: 提取整个句子特征, 实现词义理解, 长距离依赖 => 多轮对话的基础, 大模型的基础

多头注意力: 多角度语义

前馈层-残差连接: 保留原始输入 + 新的注意力输出

位置编码: 顺序感知



整个架构:

位置编码 -> 注意力编码器 -> 前馈编码器 -> 多层编码器 -> 词嵌入 -> 概率分布 -> 预测词



训练过程中一些关键指标

+ 学习率
+ 权重衰减
+ warmup
+ LayerNorm + Dropout
+ 梯度裁剪



大模型:

- Transformer attention 能**全并行 → 多卡加速训练**
- 支持长距离依赖



其他常见模型:

CNN/DCNN: 滑动窗口, 提取局部特征, 适合分类

RNN/LSTM: 短期记忆, 适合时间序列预测 (时间步, 记忆门)

diffusion: 噪声降噪, 去噪, 适合创作

xgboost: 本质是一种方法, 核心是残差逼近

gan: 生成真假样本, 对抗训练

resnet: 梯度传播的CNN, 适合深度分类

RLHF: 奖励模型, 更新权重