# 基础

## 1. “多少行、多少维”到底指什么？

无论是 PyTorch 的 `nn.Embedding`、TensorFlow 的 `Embedding`，还是各种 embedding 矩阵，本质上都是一个**二维矩阵**：

```
[ num_items, embedding_dim ]
   行数        列数
```

- **行数（num_items）**：你有多少个“东西”要表示
  - 多少个词 → 词表大小 vocab_size
  - 多少个用户 → 用户数
  - 多少个商品 → 商品数
  - 多少个类别 ID → 类别数
- **列数（embedding_dim）**：每个“东西”被表示成多长的向量
  - 比如 16维、32维、64维、128维、768维……

所以，“数据形状（多少行、多少维？）”一般就是问你：

> 你要给多少个离散ID做 embedding？每个 ID 想用多少维向量来表示？

------

## 2. 行数：根据“原始数据”的**种类数量**来定

### 情况 A：文本词向量 / 词嵌入

1. 从原始文本中分词 / token 化
2. 统计**唯一 token 数量** = 词表大小 vocab_size
   - 再加上一些特殊符号（`[PAD]`, `[UNK]` 等）

举例：
 你从所有语料里数完，一共 10,000 个不同的 token，再额外加 2 个特殊符号：

```
num_items = 10002   # 也就是“行数”
```

### 情况 B：ID 特征（用户、商品、标签…）

- 你有 1,000,000 个用户 ID
- 想给每个用户学一个向量

那：

```
num_items = 1_000_000  # 每个用户一行
```

### 情况 C：类别特征（性别、城市、职业…）

- 性别：3 种（男/女/未知） → num_items = 3
- 城市：500 个城市 → num_items = 500

**结论：**
 行数 = “你要做 embedding 的**不同 ID/类别的数量**”。

------

## 3. 列数（embedding_dim）：这是你要**人为选的超参数**

这部分是你真正纠结的：

> “我该如何根据原始数据选择合适的 embedding 维度？”

先说一个关键点：

> **embedding 维度一般跟原始数据维度没有直接线性关系**
>  它更像是“我给每个 ID 准备多大一个向量空间”，是个**可调的容量/复杂度**参数。

### 3.1 常见的经验法则（适用于 ID / 类别 特征）

常见经验（不是硬性规则）：

- 小类别数（< 100）
  - 8、16、32 维就够用了
- 中等类别数（几千～几万）
  - 16～64 维比较常见
- 超大类别数（几十万、上百万 ID，比如用户、商品等）
  - 32～128 维，甚至 256
  - 但维度太大会导致模型大、训练慢、过拟合风险增加

有些人用简单规则，比如：

- `embedding_dim ≈ min(50, 类别数/2)`
- 或 `embedding_dim ≈ 4 * sqrt(类别数)`（只是一个“感觉还行”的量级参考）

**实战建议：**
 先选一个中等值，比如 32 或 64，看效果再调。

## 4. 具体示例：从原始数据到 embedding 形状怎么走？

### 示例 1：自己定义一个文本 embedding（PyTorch 为例）

假设：

- 语料分词后有 10,000 个不同 token
- 你想每个词用 64 维向量表示

则：

```
vocab_size = 10000
embedding_dim = 64

embedding_matrix_shape = (vocab_size, embedding_dim)
# => (10000, 64)
```

代码里通常写成：

```
import torch.nn as nn

embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)
```

此时：

- “行数”= 10000
- “维度”= 64

------

### 示例 2：用户 ID Embedding

你有：

- 1,000,000 个用户
- 选 32 维作 embedding

```
num_users = 1_000_000
embedding_dim = 32
# embedding 矩阵形状：[1_000_000, 32]
```



